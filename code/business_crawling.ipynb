{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498cd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964ba4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.bbc.com\"\n",
    "start_url = \"https://www.bbc.com/news/business\"\n",
    "\n",
    "# 해당 페이지에서 각 기사들의 url 뽑아오는 함수\n",
    "def get_articles_urls(soup):\n",
    "    return [base_url + li.a[\"href\"] for li in soup.select(\"li.lx-stream__post-container\")], [li.find('article').find('div').find('div').find('time').find_all('span')[1].text for li in soup.select(\"li.lx-stream__post-container\")]\n",
    "\n",
    "\n",
    "# 해당 페이지 기사들의 정보 뽑아오는 코드\n",
    "def get_page_articles(driver):\n",
    "    driver_source = driver.page_source\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    article_urls, article_times = get_articles_urls(soup)\n",
    "\n",
    "    article_titles = []\n",
    "    article_time_save = []\n",
    "    article_contents = []\n",
    "    article_relateds = []\n",
    "    for article_url, article_time in zip(article_urls, article_times):\n",
    "        article_title, article_content, article_related = extract_single_article_content(article_url)\n",
    "        if article_title and article_content:\n",
    "            article_titles.append(article_title)\n",
    "            article_time_save.append(article_time)\n",
    "            article_contents.append(article_content)\n",
    "            article_relateds.append(article_related)\n",
    "    return article_titles, article_time_save, article_contents, article_relateds, soup\n",
    "\n",
    "\n",
    "# 한 기사의 제목, 본문, 태그들 가져오는 함수\n",
    "def extract_single_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 기사 제목 가져오기\n",
    "    title = soup.select_one(\"h1\")\n",
    "    if title:\n",
    "        title = title.get_text(strip=True)\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    # 기사 내용 가져오기\n",
    "    paragraphs = []\n",
    "    p_elements = soup.select(\"div.ssrcss-7uxr49-RichTextContainer.e5tfeyi1 > p\")\n",
    "    \n",
    "    if not p_elements: # 비어있는 경우를 확인하여 에러를 방지합니다.\n",
    "        return None, None, None\n",
    "    \n",
    "    last_p_element = p_elements[-1]\n",
    "    # div.ssrcss-7uxr49-RichTextContainer.e5tfeyi1 클래스 하위의 p 요소들을 가져오기\n",
    "    for p in p_elements:\n",
    "        # 마지막 문단의 a, i에 태그가 나올 땐 날려버려야 하는 경우가 있다.\n",
    "        if p == last_p_element:\n",
    "            # p 요소 안에 있는 모든 a와 i 태그 삭제\n",
    "            for tag in p.find_all([\"i\", \"a\"]):\n",
    "                tag.decompose()\n",
    "        # p 요소의 텍스트만 추출하여 paragraphs에 추가\n",
    "        paragraphs.append(p.text.strip())\n",
    "\n",
    "    content = \"\\n\".join(paragraphs)\n",
    "    \n",
    "    # 기사 태그 가져오기\n",
    "    tags = soup.select('div.ssrcss-1qmkvfu-TopicListWrapper.etw6iwl1 > div.ssrcss-1szabdv-StyledTagContainer.ed0g1kj1 > div.ssrcss-17ehax8-Cluster.e1ihwmse1 > ul.ssrcss-1ujonwb-ClusterItems.e1ihwmse0 > li')\n",
    "    if not tags:\n",
    "        return None, None, None\n",
    "    \n",
    "    related = ', '.join([tag.get_text() for tag in tags])\n",
    "       \n",
    "\n",
    "    return title, content, related\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모인 기사 수: 17\n",
      "모인 기사 수: 34\n",
      "모인 기사 수: 49\n"
     ]
    }
   ],
   "source": [
    "# 크롬 드라이버 설치, 연결\n",
    "s = Service(\"D:\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# 브라우저 화면 크기 변경하기\n",
    "driver.maximize_window()\n",
    "\n",
    "# 웹 페이지 열기\n",
    "url = \"https://www.bbc.com/news/business\"\n",
    "driver.get(url)\n",
    "\n",
    "# 데이터 프레임 초기화\n",
    "df_business = pd.DataFrame(columns=[\"Title\", \"Time\", \"Content\", \"Related\"])\n",
    "\n",
    "# title과 content를 가져오기 위해 get_page_articles 함수 호출\n",
    "article_titles, article_time_save, article_contents, article_relateds, soup = get_page_articles(driver)\n",
    "\n",
    "body = driver.find_elements('css selector', 'body')[0]\n",
    "for i in range(17):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    \n",
    "btn = driver.find_elements('css selector', 'div > div.gel-icon.gel-icon--next')[0]\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # 데이터 프레임에 저장\n",
    "        for title, time_save, content, related in zip(article_titles, article_time_save, article_contents, article_relateds):\n",
    "            if title in df_business:\n",
    "                continue\n",
    "            else:\n",
    "                df_business = df_business.append({\"Title\": title, \"Time\": time_save, \"Content\": content, \"Related\": related}, ignore_index=True)\n",
    "        \n",
    "        # 500개 까지만 모으기\n",
    "        if len(df_business) > 500:\n",
    "                break\n",
    "\n",
    "        print('모인 기사 수: ' + str(len(df_business)))\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        btn.click()\n",
    "\n",
    "        # 페이지를 변경한 후에도 다음 뉴스 목록을 가져올 수 있도록 변경된 페이지에 대한 soup 객체 생성\n",
    "        driver_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "\n",
    "        # 변경된 페이지에 대한 정보를 크롤링하고 순환하는 데 사용\n",
    "        article_titles, article_time_save, article_contents, aricle_relateds, soup = get_page_articles(driver)\n",
    "\n",
    "    except:\n",
    "        btn.click()\n",
    "    \n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 데이터 프레임 출력\n",
    "df_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef107a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 기사 제거\n",
    "df_business = df_business.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6548715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Time</th>\n",
       "      <th>Content</th>\n",
       "      <th>Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta OKs deleting Threads without losing Insta...</td>\n",
       "      <td>20:22</td>\n",
       "      <td>Changes are being rolled out to the Threads ap...</td>\n",
       "      <td>Social media, Meta, Instagram, Social media re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pay rises outstrip inflation by most for two y...</td>\n",
       "      <td>19:37</td>\n",
       "      <td>Wages have risen faster than inflation by the ...</td>\n",
       "      <td>Unemployment, Employment, UK economy, Pay, Off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why businesses are pulling billions in profits...</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Foreign businesses have been pulling money out...</td>\n",
       "      <td>Companies, China-US relations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cost-of-living payments: What are they, who ge...</td>\n",
       "      <td>9:03</td>\n",
       "      <td>Millions of low-income households across the U...</td>\n",
       "      <td>Pensioners, Money, Personal finance, Energy in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'We go to work to serve customers, not to be a...</td>\n",
       "      <td>1:34</td>\n",
       "      <td>Jo Crumplin, a team leader at a convenience st...</td>\n",
       "      <td>Companies, Co-operative Group, Retailing, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Mortgage rates: The full pain of this chaos is...</td>\n",
       "      <td>6:01 24 Jun</td>\n",
       "      <td>In the run-up to this week's shock interest ra...</td>\n",
       "      <td>UK Finance, Money, Personal finance, Housing m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Sunny weather sees people splash out on new cl...</td>\n",
       "      <td>21:05 23 Jun</td>\n",
       "      <td>Shoppers have been splashing out on new outfit...</td>\n",
       "      <td>Tourism, Retailing, London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Furby: Toy giant Hasbro brings back iconic rob...</td>\n",
       "      <td>20:11 23 Jun</td>\n",
       "      <td>US toy giant Hasbro has brought back the iconi...</td>\n",
       "      <td>Personal finance, Inflation, Cost of living, U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Interest rates: Bank of England boss denies wa...</td>\n",
       "      <td>2:59 23 Jun</td>\n",
       "      <td>The Bank of England boss has denied trying to ...</td>\n",
       "      <td>US economy, Airbnb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Train strikes: RMT union announces three days ...</td>\n",
       "      <td>2:07 23 Jun</td>\n",
       "      <td>Thousands of rail workers will strike on three...</td>\n",
       "      <td>Urban planning, Regeneration, Whitehaven</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title          Time  \\\n",
       "0    Meta OKs deleting Threads without losing Insta...         20:22   \n",
       "1    Pay rises outstrip inflation by most for two y...         19:37   \n",
       "2    Why businesses are pulling billions in profits...         13:00   \n",
       "3    Cost-of-living payments: What are they, who ge...          9:03   \n",
       "4    'We go to work to serve customers, not to be a...          1:34   \n",
       "..                                                 ...           ...   \n",
       "476  Mortgage rates: The full pain of this chaos is...   6:01 24 Jun   \n",
       "477  Sunny weather sees people splash out on new cl...  21:05 23 Jun   \n",
       "478  Furby: Toy giant Hasbro brings back iconic rob...  20:11 23 Jun   \n",
       "479  Interest rates: Bank of England boss denies wa...   2:59 23 Jun   \n",
       "480  Train strikes: RMT union announces three days ...   2:07 23 Jun   \n",
       "\n",
       "                                               Content  \\\n",
       "0    Changes are being rolled out to the Threads ap...   \n",
       "1    Wages have risen faster than inflation by the ...   \n",
       "2    Foreign businesses have been pulling money out...   \n",
       "3    Millions of low-income households across the U...   \n",
       "4    Jo Crumplin, a team leader at a convenience st...   \n",
       "..                                                 ...   \n",
       "476  In the run-up to this week's shock interest ra...   \n",
       "477  Shoppers have been splashing out on new outfit...   \n",
       "478  US toy giant Hasbro has brought back the iconi...   \n",
       "479  The Bank of England boss has denied trying to ...   \n",
       "480  Thousands of rail workers will strike on three...   \n",
       "\n",
       "                                               Related  \n",
       "0    Social media, Meta, Instagram, Social media re...  \n",
       "1    Unemployment, Employment, UK economy, Pay, Off...  \n",
       "2                        Companies, China-US relations  \n",
       "3    Pensioners, Money, Personal finance, Energy in...  \n",
       "4      Companies, Co-operative Group, Retailing, Crime  \n",
       "..                                                 ...  \n",
       "476  UK Finance, Money, Personal finance, Housing m...  \n",
       "477                         Tourism, Retailing, London  \n",
       "478  Personal finance, Inflation, Cost of living, U...  \n",
       "479                                 US economy, Airbnb  \n",
       "480           Urban planning, Regeneration, Whitehaven  \n",
       "\n",
       "[481 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "347900d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.to_excel('business.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
