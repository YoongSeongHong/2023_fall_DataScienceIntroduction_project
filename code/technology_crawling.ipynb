{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994e4a3f",
   "metadata": {},
   "source": [
    "##  11월 14일 크롤링 진행하여 데이터 수집했는데, 이후 웹페이지 구조가 바뀌어서 코드가 정상적으로 재실행 되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7455aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e8d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.bbc.com\"\n",
    "start_url = \"https://www.bbc.com/news/technology\"\n",
    "\n",
    "# 해당 페이지에서 각 기사들의 url 뽑아오는 함수\n",
    "def get_articles_urls(soup):\n",
    "    return [base_url + li.a[\"href\"] for li in soup.select(\"li.lx-stream__post-container\")], [li.find('article').find('div').find('div').find('time').find_all('span')[1].text for li in soup.select(\"li.lx-stream__post-container\")]\n",
    "\n",
    "\n",
    "# 해당 페이지 기사들의 정보 뽑아오는 코드\n",
    "def get_page_articles(driver):\n",
    "    driver_source = driver.page_source\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    article_urls, article_times = get_articles_urls(soup)\n",
    "\n",
    "    article_titles = []\n",
    "    article_time_save = []\n",
    "    article_contents = []\n",
    "    article_relateds = []\n",
    "    for article_url, article_time in zip(article_urls, article_times):\n",
    "        article_title, article_content, article_related = extract_single_article_content(article_url)\n",
    "        if article_title and article_content:\n",
    "            article_titles.append(article_title)\n",
    "            article_time_save.append(article_time)\n",
    "            article_contents.append(article_content)\n",
    "            article_relateds.append(article_related)\n",
    "    return article_titles, article_time_save, article_contents, article_relateds, soup\n",
    "\n",
    "\n",
    "# 한 기사의 제목, 본문, 태그들 가져오는 함수\n",
    "def extract_single_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 기사 제목 가져오기\n",
    "    title = soup.select_one(\"h1\")\n",
    "    if title:\n",
    "        title = title.get_text(strip=True)\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    # 기사 내용 가져오기\n",
    "    paragraphs = []\n",
    "    p_elements = soup.select(\"div.ssrcss-7uxr49-RichTextContainer.e5tfeyi1 > p\")\n",
    "    \n",
    "    if not p_elements: # 비어있는 경우를 확인하여 에러를 방지합니다.\n",
    "        return None, None, None\n",
    "    \n",
    "    last_p_element = p_elements[-1]\n",
    "    # div.ssrcss-7uxr49-RichTextContainer.e5tfeyi1 클래스 하위의 p 요소들을 가져오기\n",
    "    for p in p_elements:\n",
    "        # 마지막 문단의 a, i에 태그가 나올 땐 날려버려야 하는 경우가 있다.\n",
    "        if p == last_p_element:\n",
    "            # p 요소 안에 있는 모든 a와 i 태그 삭제\n",
    "            for tag in p.find_all([\"i\", \"a\"]):\n",
    "                tag.decompose()\n",
    "        # p 요소의 텍스트만 추출하여 paragraphs에 추가\n",
    "        paragraphs.append(p.text.strip())\n",
    "\n",
    "    content = \"\\n\".join(paragraphs)\n",
    "    \n",
    "    # 기사 태그 가져오기\n",
    "    tags = soup.select('div.ssrcss-1qmkvfu-TopicListWrapper.etw6iwl1 > div.ssrcss-1szabdv-StyledTagContainer.ed0g1kj1 > div.ssrcss-17ehax8-Cluster.e1ihwmse1 > ul.ssrcss-1ujonwb-ClusterItems.e1ihwmse0 > li')\n",
    "    if not tags:\n",
    "        return None, None, None\n",
    "    \n",
    "    related = ', '.join([tag.get_text() for tag in tags])\n",
    "       \n",
    "\n",
    "    return title, content, related\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e999bfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m17\u001b[39m):\n\u001b[0;32m     20\u001b[0m     body\u001b[38;5;241m.\u001b[39msend_keys(Keys\u001b[38;5;241m.\u001b[39mPAGE_DOWN)\n\u001b[1;32m---> 22\u001b[0m btn \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcss selector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv > div.gel-icon.gel-icon--next\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# 데이터 프레임에 저장\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 크롬 드라이버 설치, 연결\n",
    "s = Service(\"D:\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# 브라우저 화면 크기 변경하기\n",
    "driver.maximize_window()\n",
    "\n",
    "# 웹 페이지 열기\n",
    "url = \"https://www.bbc.com/news/technology\"\n",
    "driver.get(url)\n",
    "\n",
    "# 데이터 프레임 초기화\n",
    "df_tech = pd.DataFrame(columns=[\"Title\", \"Time\", \"Content\", \"Related\"])\n",
    "\n",
    "# title과 content를 가져오기 위해 get_page_articles 함수 호출\n",
    "article_titles, article_time_save, article_contents, article_relateds, soup = get_page_articles(driver)\n",
    "\n",
    "body = driver.find_elements('css selector', 'body')[0]\n",
    "for i in range(17):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    \n",
    "btn = driver.find_elements('css selector', 'div > div.gel-icon.gel-icon--next')[0]\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # 데이터 프레임에 저장\n",
    "        for title, time_save, content, related in zip(article_titles, article_time_save, article_contents, article_relateds):\n",
    "            if title in df_tech:\n",
    "                continue\n",
    "            else:\n",
    "                df_tech = df_tech.append({\"Title\": title, \"Time\": time_save, \"Content\": content, \"Related\": related}, ignore_index=True)\n",
    "        \n",
    "        # 500개 까지만 모으기\n",
    "        if len(df_tech) > 500:\n",
    "                break\n",
    "\n",
    "        print('모인 기사 수: ' + str(len(df_tech)))\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        btn.click()\n",
    "\n",
    "        # 페이지를 변경한 후에도 다음 뉴스 목록을 가져올 수 있도록 변경된 페이지에 대한 soup 객체 생성\n",
    "        driver_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "\n",
    "        # 변경된 페이지에 대한 정보를 크롤링하고 순환하는 데 사용\n",
    "        article_titles, article_time_save, article_contents, aricle_relateds, soup = get_page_articles(driver)\n",
    "\n",
    "    except:\n",
    "        btn.click()\n",
    "    \n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 데이터 프레임 출력\n",
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67fd0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 기사 제거\n",
    "df_tech = df_tech.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2357d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Time</th>\n",
       "      <th>Content</th>\n",
       "      <th>Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Time, Content, Related]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f96871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech.to_excel('technology.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
